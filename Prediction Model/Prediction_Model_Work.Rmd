---
title: "Capstone Prediction Model Development"
output: html_notebook
---

#Build model
## Tasks to accomplish

1. Build a predictive model based on the previous data modeling steps - you may combine the models in any way you think is appropriate.
2. Evaluate the model for efficiency and accuracy - use timing software to evaluate the computational complexity of your model. Evaluate the model accuracy using different metrics like perplexity, accuracy at the first word, second word, and third word.

## Questions to consider

1. How does the model perform for different choices of the parameters and size of the model?
2. How much does the model slow down for the performance you gain?
3. Does perplexity correlate with the other measures of accuracy?
4. Can you reduce the size of the model (number of parameters) without reducing performance?

### Pull in data

Let's get the data in again

```{r, warning=FALSE, cache=TRUE, message = FALSE}
setwd("~/JHU_DataScienceCapstone")
con <- file("Data/en_US.twitter.txt", "rb")
twitterData <- readLines(con)
close(con)
con <- file("Data/en_US.blogs.txt", "rb")
blogData <- readLines(con)
close(con)
con <- file("Data/en_US.news.txt", "rb")
newsData <- readLines(con)
close(con)
```


### Sample data down

Previously I pulled the data down to 20k rows for each text source type.  That's less then 1% for twitter data up to 2-3% for blog data. This time I'll start with 2% of all data types, and if that's too much to process then I'll go down from there:

```{r, warning=FALSE, cache=TRUE, message = FALSE}
set.seed(2019-9-5)
twitterData <- twitterData[sample(1:length(twitterData),round(length(twitterData)*.02,0))]
blogData <- blogData[sample(1:length(blogData),round(length(blogData)*.02,0))]
newsData <- newsData[sample(1:length(newsData),round(length(newsData)*.02,0))]
```

### n-grams

```{r, warning=FALSE, cache=TRUE, message = FALSE}
library(tm)
library(slam)

corpusTwitter <- VCorpus(VectorSource(twitterData))
corpusBlog <- VCorpus(VectorSource(blogData))
corpusNews <- VCorpus(VectorSource(newsData))

corpusTwitter <- tm_map(corpusTwitter, stripWhitespace)
corpusBlog <- tm_map(corpusBlog, stripWhitespace)
corpusNews <- tm_map(corpusNews, stripWhitespace)

DTMtwitter_uni <- DocumentTermMatrix(corpusTwitter)
DTMblog_uni <- DocumentTermMatrix(corpusBlog)
DTMnews_uni <- DocumentTermMatrix(corpusNews)

BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

DTMtwitter_bi <- DocumentTermMatrix(corpusTwitter, control=list(tokenize=BigramTokenizer))
DTMblog_bi <- DocumentTermMatrix(corpusBlog, control=list(tokenize=BigramTokenizer))
DTMnews_bi <- DocumentTermMatrix(corpusNews, control=list(tokenize=BigramTokenizer))

DTMtwitter_tri <- DocumentTermMatrix(corpusTwitter, control=list(tokenize=TrigramTokenizer))
DTMblog_tri <- DocumentTermMatrix(corpusBlog, control=list(tokenize=TrigramTokenizer))
DTMnews_tri <- DocumentTermMatrix(corpusNews, control=list(tokenize=TrigramTokenizer))

```

I could keep from re-inventing the wheel by seeing what other people have implemented, but I'd like to see what I can do on my own in this space.











```{r, warning=FALSE, cache=TRUE, message = FALSE}
unigs <- tokens_ngrams(tokens(str_c(c(blogchar,twitterchar,newschar),collapse = " ")),n=1)[[1]]
unigs <- count(unigs)
unigs <- unigs[order(-unigs$freq),]
row.names(unigs) <- 1:nrow(unigs)
bigrs <- tokens_ngrams(tokens(str_c(c(blogchar,twitterchar,newschar),collapse = " ")),n=2)[[1]]
bigrs <- count(bigrs)
bigrs <- bigrs[order(-bigrs$freq),]
row.names(bigrs) <- 1:nrow(bigrs)
trigs <- tokens_ngrams(tokens(str_c(c(blogchar,twitterchar,newschar),collapse = " ")),n=3)[[1]]
trigs <- count(trigs)
trigs <- trigs[order(-trigs$freq),]
row.names(trigs) <- 1:nrow(trigs)
```





## Tasks to accomplish

1. Explore new models and data to improve your predictive model.
2. Evaluate your new predictions on both accuracy and efficiency.

## Questions to consider

1. What are some alternative data sets you could consider using?
2. What are ways in which the n-gram model may be inefficient?
3. What are the most commonly missed n-grams? Can you think of a reason why they would be missed and fix that?
4. What are some other things that other people have tried to improve their model?
5. Can you estimate how uncertain you are about the words you are predicting?

## Tasks to accomplish

1. Create a data product to show off your prediction algorithm You should create a Shiny app that accepts an n-gram and predicts the next word.

## Questions to consider

1. What are the most interesting ways you could show off your algorithm?
2. Are there any data visualizations you think might be helpful (look at the Swiftkey data dashboard if you have it loaded on your phone)?
3. How should you document the use of your data product (separately from how you created it) so that others can rapidly deploy your algorithm?

## Tips, tricks, and hints

1. Consider the size of the predictive model you have developed. You may have to sacrifice some accuracy to have a fast enough/small enough model to load into Shiny.


## Tasks to accomplish

1. Create a slide deck promoting your product. Write 5 slides using RStudio Presenter explaining your product and why it is awesome!

## Questions to consider

1. How can you briefly explain how your predictive model works?
2. How can you succinctly quantitatively summarize the performance of your prediction algorithm?
3. How can you show the user how the product works?

## Tips, tricks, and hints

1. The Rstudio presentation information is available here ([https://support.rstudio.com/hc/en-us/articles/200486468-Authoring-R-Presentations](https://support.rstudio.com/hc/en-us/articles/200486468-Authoring-R-Presentations)).